# Method and Materials

A sample of texts represents significant dimensional space to produce high accuracies for binary sentiment classification. 
The text samples represent feature space for the set task. 
The words in the language determine the text sample's sentiment based on a polarity measure calculated on the document level; otherwise, the binary classification of the texts is provided with the dataset. 

A supervised machine learning algorithm, a deep neural network known as a C-LSTM, a hybrid neural network that includes convolutional and long-short-term-memory layers, makes it possible to extract the sentiment from the text alone. 
Applying adaptable parameters and structures within the C-LSTM layers provides an architecture capable of reliably performing text classification tasks. 
The PTCN or *"Predicted Text Classification Network"* is a reliable model for text classification [@jkmn].
All modeling efforts are captured in the following order: 

1)	Text Pre-Processing & Sentiment Labeling
2)	Hyper-Tuning of the PTCN & Best Parameters PTCN Model 10-fold Cross-validation

## Text Pre-Processing & Sentiment Labeling

A series of natural language processing techniques are required to supervise the training of the PTCN to classify the text sentiment. 
The text samples contain noise, which is in the form of stop words, uppercasing, special characters, NA values, punctuation, numbers, and whitespace [@jkmn]. 
The removal of noise from the text samples helps mitigate the C-LSTM, recognizing patterns within the texts that create bias predictions. 
Text samples with mitigated noise are not the only requirement for training the PTCN. 
Another important factor is to create an equal distribution of the labels contained in the sample data. 
DL models easily form a bias if the representation of the targets is not controlled. 

A classification of each of the text samples' binary sentiment is needed. 
If necessary, the label of sentiment for each text sample is completed by calculating each word's polarity in a text sample and concluding the overall polarity score. 
The overall polarity score determines the classification of sentiment for each text based on a conditional threshold of above or below a neutral value of 0. 
If the polarity is above 0, the classification label is positive for that text sample. 
If the value is below 0, the classification label is negative. 
Each polarity measure is converted into a binary label depending on the conditional threshold.
Otherwise, the data source provided the labels for the sentiment. 

## Hyper-Tuning of the PTCN and Best Parameters PTCN Model 10-fold Cross-validation

The PTCN is a deep learning algorithm that automatically extracts features from an input vector with adaptable kernel initializers. 
An additional batch normalization layer and dropout layer are implemented to avoid bias in the model's predictions. 
Deep neural networks transform inputs over extensive credit assignment pathways *(CAPs)*. 
The more complicated the dimensions of input, then the more complex the CAPs. 
The labeled sentiment text samples are balanced into an equal distribution to confirm that the features extracted throughout the text inputs' transformation are only significant. 
The C-LSTM will be trained on an equal amount of random positive sentiment text samples and random negative sentiment text samples. 
During the hyper-tuning permutations, the samples are embedded into the network's layers in the form of a tokenized word vectors. 
The space of the tokenized word vector represents the positive and negative texts that the model will extract features from to form accurate predictions. 
A list of input samples is tokenized in sequence to form numerical values for each represented word across the texts. 
Each text is transformed into a numerical sequence of tokenized words in the form of vectors. 
The newly formed word vector spaces are attached to their designated sentiment label. 
The lists of input vectors and output scalars require further transformation to embed within the PTCN.
The text inputs are padded to make each text the same size. 
The list of padded inputs is transformed into a matrix. 
The outputs are converted into categorical variables for each sample transforming from a list of scalars into a matrix of binary responses. 
The complete sample of text and sentiment labels are split for training, validation, and evaluation datasets during the hyper-tuning sessions. 
A fixed set of samples is reserved for evaluating the model's hyper-tuning permutations and their corresponding parameters. 
Once the number of permutations is complete, a set of the best parameters are captured based on the highest evaluation accuracy. 
The best parameters are stored for training the best model. 
For the best model, a small variation is required to process the sentiment labeled text samples. 
Texts processing techniques all remain the same, but the sampling for training and validation sets is changed to incorporate $10$-fold cross-validation. 

After hyper-tuning, the best model is initiated. 
The PTCN embeds the pre-processed texts into the model's layers and transforms the inputs through the stacks. 
The inputs are embedded based on their dimensions, including the max features, embedding dimensions, and max length of the inputs. 
The inputs are piped through a series of layers in the model. 1-dimensional convolutional layers are set with a series of elements to control various parameters of the convolutions. 
The input samples are large vectors with high sparsity, requiring that the layers be allowed to explore but in a controlled manner. 
A series of convolutional layers are stacked and activated using a leaky Relu function to promote the model identifying patterns in the inputs. 
The model does not overfit because max-pooling layers are implemented at specific points in the model's architecture. 
Heavy regularization techniques using Lasso and Ridge regression are implemented to mitigate overfitting or underfitting further. 
The model is designed to pipe the inputs through a series of convolutional layers and then into a long-short-term-memory (LSTM) layer set with various parameter controls and activated with a leaky Relu function. 
In the last layer, the inputs are piped into a fully connected layer activated using a sigmoid function. 
The model is compiled using a binary cross-entropy function, optimized with a stochastic gradient descent *(SGD)* function, and reporting results in set metrics.

The model is fit to the training dataset of inputs and outputs through a set number of epochs. 
Deep learning models validate after each epoch. 
A specific percentage of the training data is set for validation. 
To ensure the model does not train further than it can learn, early stopping parameters are utilized to automate a change in the optimizer function's learning rate and end the model's training session if improvement plateaus after a set number of epochs. 
The best weights are captured and set for the final model state that is saved for evaluation. 
The evaluation dataset is tested using the fitted model and determines the model's ability to generalize to new inputs. 
Hyper-tuning the model's parameters will ensure the best possible values are set for final modeling. 
The final model is trained using 10-fold cross-validation to understand the predictive capabilities of the model fully. To ensure that the model understands the language after the best model is captured and cross-validated, visualization of the embedded features provides insight.
The features embedded in the network's layers are visualized to express the words that support or contradict the predicted sentiment classification.
Understanding the features helps determine the usefulness of the convolutions recognizing characteristics from the word vector spaces. 

The layers throughout the network's stack are essential to the performance of the PCTN. 
The sequential deep learning model is constructed with a stack of different layers set with several parameters, including: dropout rate, convolutional hidden nodes, LSTM hidden nodes, L1 regularization rate, L2 regularization rate, batch size, input max length, max features, embedding dimensions, Leaky Relu rate, kernel size, epochs, max-pooling size, learning rate, and validation split. 

The first layer in the model is the embedding layer, which embeds the tokenized words into the network. 
Word vector spaces reflect common practice in natural language processing. 
The convolutional and LSTM layers in the architecture can extract features automatically from the highly sparse dimensional space of the processed inputs. 
The next layer the inputs are piped into is a 1-dimensional convolutional layer with only a 4th of the set convolutional hidden nodes. 
The inputs are representations of long and highly sparse word vector spaces, which makes the model weights critically important.
The model requires a method to adjust to the high variance between samples.
The control of the kernels will help the model steadily learn from the features extracted from the text. 

The use of a variance scaling kernel initializer in the network provides a process *"... that is capable of adapting its scale to the shape of weights."* [@conv1d]. 
It is important to regularize the kernels when utilizing an adaptive initializer. 
L1 and L2 regularization help mitigate high fluctuations while batching samples through the layers. L1 is Lasso Regression (LR). 
It is a penalty term measuring the *"... absolute value of the magnitude of the coefficient."* [@regularizer]. 
L2 is Ridge Regression. 
A penalty term measuring the *"... squared magnitude of the coefficient."* [@regularizer]. 
The main difference between the two methods is the penalty term. The 1-dimensional convolutional layers are set to inputs padded to the same shape. 
Strides are set to 1L through the convolutions, which are *"... an integer or list of a single integer, specifying the stride length of the convolution.* [*(R Documentation, 2019)* [@conv1dlayer]. 
The convolutional layer is activated using a Leaky Relu function. 
The leaky relu function allows for a *"... small gradient when the unit is not active... "*, providing *"... slightly higher flexibility to the model than traditional Relu."* [@lrelu].
The first convolutional layer extracts lower level features from the inputs due to the decrease in the hidden nodes.
The reduction of the number of transformations provides control to the adaptable features initializing the weights. 
A high kernel size provides the layer with the ability to transform the data at a higher level through a few transformations. 
The second layer piped to is another 1-dimensional convolutional layer with the same parameters set, except that the number of hidden nodes is set to 32. 
Increasing the number of hidden nodes provides more transformations extracting higher-level features from the inputs. 
The second convolutional layer is activated using the leaky Relu function. 
The next layer in the stack is another convolutional layer set at half the set number of hidden nodes. 

Overfitting is mitigated in the convolutional layers by using a max-pooling layer set to 4. 
Following are two more layers of 1-dimensional convolutional layers set to half the number of set hidden nodes and a 4th of the set hidden nodes. 
All parameters are set the same as the prior convolutional layers. 
A second max-pooling layer, batch normalization, and dropout layer are set to mitigate overfitting further. 
The next layer is an LSTM layer with the set at 32 hidden nodes. Variance Scaling kernel initializers and L1 and L2 kernel regularizers are implemented. 
The LSTM layer is activated using a Leaky Relu function. 
A dropout layer set at 0.5 is implemented before the output layer. The output layer is activated using a sigmoid function for binary problems and a softmax function if it is multi-class. 
The model is compiled using a loss function of binary cross-entropy for binary classification problems or a categorical cross-entropy function multi-classification problems. 
The stochastic gradient descent (SGD) optimizer is set with a learning rate at a specific value based on the hyper-tuning sessions. 
The learning rate will be adjusted to different rates during the hyper-tuning sessions to provide the best learning rate for the final model. 
The models will provide training, validation, and evaluation metrics in the form of accuracy percentages. 
The results will also provide the loss measure of the training, validation, and evaluation. 

In Section \@ref(results), the PTCN is tested on a new data sample, the Movie Scripts, and compared to the performance of other deep models aimed at sentiment classification. 
